{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with entropy, information gain, and decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris fact of the day: Iris setosa's root contains a toxin that was used by the Aleut tribe in Alaska to make poisonous arrowheads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# For producing decision tree diagrams.\n",
    "from IPython.core.display import Image, display\n",
    "from sklearn.externals.six import StringIO\n",
    "import graphviz\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris target names: ['setosa' 'versicolor' 'virginica']\n",
      "Iris feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "# Load the data, which is included in sklearn.\n",
    "iris = load_iris()\n",
    "print 'Iris target names:', iris.target_names\n",
    "print 'Iris feature names:', iris.feature_names\n",
    "X, Y = iris.data, iris.target\n",
    "\n",
    "# Shuffle the data, but make sure that the features and accompanying labels stay in sync.\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "\n",
    "# Split into train and test.\n",
    "train_data, train_labels = X[:100], Y[:100]\n",
    "test_data, test_labels = X[100:], Y[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.8, 2.8, 5.1, 2.4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that applies a threshold to turn real valued iris features into 0/1 features.\n",
    "# 0 will mean \"short\" and 1 will mean \"long\".\n",
    "def binarize_iris(data, thresholds=[6.0, 3.0, 2.5, 1.0]):\n",
    "    # Initialize a new feature array with the same shape as the original data.\n",
    "    binarized_data = np.zeros(data.shape)\n",
    "\n",
    "    # Apply a threshold  to each feature.\n",
    "    for feature in range(data.shape[1]):\n",
    "        binarized_data[:,feature] = data[:,feature] > thresholds[feature]\n",
    "    return binarized_data\n",
    "\n",
    "# Create new binarized training and test data\n",
    "binarized_train_data = binarize_iris(train_data)\n",
    "binarized_test_data = binarize_iris(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 1.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarized_train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining a function that computes the entropy of a distribution. Remember that entropy is a measure of uncertainty. It is maximized when the distribution is uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(distribution):\n",
    "    h = 0.0\n",
    "    for probability in distribution:\n",
    "        logprob = -100.0  # log(0) = -inf so let's approximate it with -100 to avoid an error\n",
    "        if probability > 0.0: logprob = np.log2(probability)\n",
    "        h -= probability * logprob\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.02040816, 0.04081633, 0.06122449, 0.08163265,\n",
       "       0.10204082, 0.12244898, 0.14285714, 0.16326531, 0.18367347,\n",
       "       0.20408163, 0.2244898 , 0.24489796, 0.26530612, 0.28571429,\n",
       "       0.30612245, 0.32653061, 0.34693878, 0.36734694, 0.3877551 ,\n",
       "       0.40816327, 0.42857143, 0.44897959, 0.46938776, 0.48979592,\n",
       "       0.51020408, 0.53061224, 0.55102041, 0.57142857, 0.59183673,\n",
       "       0.6122449 , 0.63265306, 0.65306122, 0.67346939, 0.69387755,\n",
       "       0.71428571, 0.73469388, 0.75510204, 0.7755102 , 0.79591837,\n",
       "       0.81632653, 0.83673469, 0.85714286, 0.87755102, 0.89795918,\n",
       "       0.91836735, 0.93877551, 0.95918367, 0.97959184, 1.        ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show a plot of the entropy, H(X), of a Bernoulli random variable X.\n",
    "p_values = np.linspace(0, 1, 50)\n",
    "p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.14372616993938178,\n",
       " 0.2460225782203316,\n",
       " 0.3322866302216151,\n",
       " 0.40790598013897633,\n",
       " 0.47543164626872064,\n",
       " 0.5363596511250623,\n",
       " 0.5916727785823273,\n",
       " 0.6420642892306891,\n",
       " 0.6880476235340796,\n",
       " 0.7300166301457938,\n",
       " 0.7682814090975241,\n",
       " 0.8030909760146978,\n",
       " 0.8346482851813828,\n",
       " 0.863120568566631,\n",
       " 0.8886466698980792,\n",
       " 0.9113423759758416,\n",
       " 0.9313043685793763,\n",
       " 0.9486131982385806,\n",
       " 0.9633355456726841,\n",
       " 0.9755259511264972,\n",
       " 0.9852281360342516,\n",
       " 0.9924760039430818,\n",
       " 0.997294381646235,\n",
       " 0.999699542856517,\n",
       " 0.999699542856517,\n",
       " 0.997294381646235,\n",
       " 0.992476003943082,\n",
       " 0.9852281360342516,\n",
       " 0.9755259511264971,\n",
       " 0.9633355456726842,\n",
       " 0.9486131982385806,\n",
       " 0.9313043685793763,\n",
       " 0.9113423759758417,\n",
       " 0.8886466698980793,\n",
       " 0.8631205685666311,\n",
       " 0.834648285181383,\n",
       " 0.8030909760146978,\n",
       " 0.7682814090975241,\n",
       " 0.730016630145794,\n",
       " 0.6880476235340798,\n",
       " 0.6420642892306894,\n",
       " 0.5916727785823275,\n",
       " 0.5363596511250625,\n",
       " 0.47543164626872103,\n",
       " 0.40790598013897666,\n",
       " 0.3322866302216154,\n",
       " 0.24602257822033174,\n",
       " 0.14372616993938186,\n",
       " 0.0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropies = [entropy([p, 1-p]) for p in p_values]\n",
    "entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(p_values, entropies, 'o')\n",
    "plt.xlabel('P(X=1)')\n",
    "plt.ylabel('H(X)')\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the entropy of our distribution over labels. You may recall that the distribution in the training data is nearly uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_distribution(labels):\n",
    "    # Initialize counters for all labels to zero.\n",
    "    label_probs = np.array([0.0 for i in range(len(iris.target_names))])\n",
    "\n",
    "    # Iterate over labels in the training data and update counts.\n",
    "    for label in labels:\n",
    "        label_probs[label] += 1.0\n",
    "    \n",
    "    # Normalize to get a distribution.\n",
    "    label_probs /= label_probs.sum()\n",
    "    return label_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution [0.31 0.33 0.36]\n",
      "Label entropy: 1.5822322736472714\n",
      "Uniform entropy: 1.584962500721156\n"
     ]
    }
   ],
   "source": [
    "label_probs = get_label_distribution(train_labels)\n",
    "print 'Label distribution', label_probs\n",
    "\n",
    "# Compare the label entropy to a uniform distribution.\n",
    "print 'Label entropy:', entropy(label_probs)\n",
    "print 'Uniform entropy:', entropy([1./3, 1./3, 1./3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's figure out which feature provides the greatest information gain. To do this, we need to look at the entropy of each subset of the labels after splitting on each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.406 sepal length (cm)\n",
      "1 0.216 sepal width (cm)\n",
      "2 0.893 petal length (cm)\n",
      "3 0.780 petal width (cm)\n"
     ]
    }
   ],
   "source": [
    "# A function that computes information gain given these inputs:\n",
    "#   data: an array of featurized examples\n",
    "#   labels: an array of labels corresponding to the the data\n",
    "#   feature: the feature to use to split the data\n",
    "#   threshold: the feature value to use to split the data (the default threshold is good for binary features)\n",
    "def information_gain(data, labels, feature, threshold=0):\n",
    "    # Get the initial entropy of the label distribution.\n",
    "    initial_entropy = entropy(get_label_distribution(labels))\n",
    "    \n",
    "    # subset0 will contain the labels for which the feature is 0 and\n",
    "    # subset1 will contain the labels for which the feature is 1.\n",
    "    subset0, subset1 = [], []\n",
    "    for datum, label in zip(data, labels):\n",
    "        if datum[feature] > threshold: subset1.append(label)\n",
    "        else: subset0.append(label)\n",
    "    \n",
    "    # Compute the entropy of each subset.\n",
    "    subset0_entropy = entropy(get_label_distribution(subset0))\n",
    "    subset1_entropy = entropy(get_label_distribution(subset1))\n",
    "    \n",
    "    # Compute the final entropy by weighting each subset's entropy according to its size.\n",
    "    subset0_weight = 1.0 * len(subset0) / len(labels)\n",
    "    subset1_weight = 1.0 * len(subset1) / len(labels)\n",
    "    final_entropy = subset0_weight * subset0_entropy + subset1_weight * subset1_entropy\n",
    "    \n",
    "    # Finally, compute information gain as the difference between the initial and final entropy.\n",
    "    return initial_entropy - final_entropy\n",
    "\n",
    "for feature in range(binarized_train_data.shape[1]):\n",
    "    ig = information_gain(binarized_train_data, train_labels, feature)\n",
    "    print '%d %.3f %s' %(feature, ig, iris.feature_names[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the information gain metric, petal length is the most useful feature, followed by petal width. Let's confirm that this agrees with the sklearn decision tree implementation. Actually, sklearn doesn't expose the information gain values. Instead, it stores the a distribution of \"feature importances\", which reflects the value of each feature in the full decision tree. Let's train a decision tree with max_depth=1 so it will only choose a single feature. Let's also get the test accuracy with this \"decision stump\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DecisionTreeClassifier in module sklearn.tree.tree:\n",
      "\n",
      "class DecisionTreeClassifier(BaseDecisionTree, sklearn.base.ClassifierMixin)\n",
      " |  A decision tree classifier.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <tree>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  criterion : string, optional (default=\"gini\")\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      " |  \n",
      " |  splitter : string, optional (default=\"best\")\n",
      " |      The strategy used to choose the split at each node. Supported\n",
      " |      strategies are \"best\" to choose the best split and \"random\" to choose\n",
      " |      the best random split.\n",
      " |  \n",
      " |  max_depth : int or None, optional (default=None)\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int, float, optional (default=2)\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a percentage and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for percentages.\n",
      " |  \n",
      " |  min_samples_leaf : int, float, optional (default=1)\n",
      " |      The minimum number of samples required to be at a leaf node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a percentage and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for percentages.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : int, float, string or None, optional (default=None)\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |          - If int, then consider `max_features` features at each split.\n",
      " |          - If float, then `max_features` is a percentage and\n",
      " |            `int(max_features * n_features)` features are considered at each\n",
      " |            split.\n",
      " |          - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |          - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |          - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |          - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  max_leaf_nodes : int or None, optional (default=None)\n",
      " |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, optional (default=0.)\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float,\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\n",
      " |         Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  class_weight : dict, list of dicts, \"balanced\" or None, default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      Note that for multioutput (including multilabel) weights should be\n",
      " |      defined for each class of every column in its own dict. For example,\n",
      " |      for four-class multilabel classification weights should be\n",
      " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  presort : bool, optional (default=False)\n",
      " |      Whether to presort the data to speed up the finding of best splits in\n",
      " |      fitting. For the default settings of a decision tree on large\n",
      " |      datasets, setting this to true may slow down the training process.\n",
      " |      When using either a smaller dataset or a restricted depth, this may\n",
      " |      speed up the training.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : array of shape = [n_classes] or a list of such arrays\n",
      " |      The classes labels (single output problem),\n",
      " |      or a list of arrays of class labels (multi-output problem).\n",
      " |  \n",
      " |  feature_importances_ : array of shape = [n_features]\n",
      " |      The feature importances. The higher, the more important the\n",
      " |      feature. The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance [4]_.\n",
      " |  \n",
      " |  max_features_ : int,\n",
      " |      The inferred value of max_features.\n",
      " |  \n",
      " |  n_classes_ : int or list\n",
      " |      The number of classes (for single output problems),\n",
      " |      or a list containing the number of classes for each\n",
      " |      output (for multi-output problems).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  tree_ : Tree object\n",
      " |      The underlying Tree object.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data and\n",
      " |  ``max_features=n_features``, if the improvement of the criterion is\n",
      " |  identical for several splits enumerated during the search of the best\n",
      " |  split. To obtain a deterministic behaviour during fitting,\n",
      " |  ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  DecisionTreeRegressor\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      " |  \n",
      " |  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
      " |         and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
      " |  \n",
      " |  .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
      " |         Learning\", Springer, 2009.\n",
      " |  \n",
      " |  .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
      " |         http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.model_selection import cross_val_score\n",
      " |  >>> from sklearn.tree import DecisionTreeClassifier\n",
      " |  >>> clf = DecisionTreeClassifier(random_state=0)\n",
      " |  >>> iris = load_iris()\n",
      " |  >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
      " |  ...                             # doctest: +SKIP\n",
      " |  ...\n",
      " |  array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n",
      " |          0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DecisionTreeClassifier\n",
      " |      BaseDecisionTree\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted=None)\n",
      " |      Build a decision tree classifier from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The target values (class labels) as integers or strings.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples] or None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. Splits are also\n",
      " |          ignored if they would result in any single class carrying a\n",
      " |          negative weight in either child node.\n",
      " |      \n",
      " |      check_input : boolean, (default=True)\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      X_idx_sorted : array-like, shape = [n_samples, n_features], optional\n",
      " |          The indexes of the sorted training input samples. If many tree\n",
      " |          are grown on the same dataset, this allows the ordering to be\n",
      " |          cached between trees. If None, the data will be sorted here.\n",
      " |          Don't use this parameter unless you know what to do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities of the input samples X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class log-probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute `classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X, check_input=True)\n",
      " |      Predict class probabilities of the input samples X.\n",
      " |      \n",
      " |      The predicted class probability is the fraction of samples of the same\n",
      " |      class in a leaf.\n",
      " |      \n",
      " |      check_input : boolean, (default=True)\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool\n",
      " |          Run check_array on X.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute `classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset([])\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  apply(self, X, check_input=True)\n",
      " |      Returns the index of the leaf that each sample is predicted as.\n",
      " |      \n",
      " |      .. versionadded:: 0.17\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : boolean, (default=True)\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape = [n_samples,]\n",
      " |          For each datapoint x in X, return the index of the leaf x\n",
      " |          ends up in. Leaves are numbered within\n",
      " |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
      " |          numbering.\n",
      " |  \n",
      " |  decision_path(self, X, check_input=True)\n",
      " |      Return the decision path in the tree\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : boolean, (default=True)\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      " |          Return a node indicator matrix where non zero elements\n",
      " |          indicates that the samples goes through the nodes.\n",
      " |  \n",
      " |  predict(self, X, check_input=True)\n",
      " |      Predict class or regression value for X.\n",
      " |      \n",
      " |      For a classification model, the predicted class for each sample in X is\n",
      " |      returned. For a regression model, the predicted value based on X is\n",
      " |      returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : boolean, (default=True)\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The predicted classes, or the predict values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances.\n",
      " |      \n",
      " |      The importance of a feature is computed as the (normalized) total\n",
      " |      reduction of the criterion brought by that feature.\n",
      " |      It is also known as the Gini importance.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array, shape = [n_features]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a decision stump -- a tree with depth 1:\n",
      "Feature importances: [0.09092576 0.0050933  0.89011686 0.01386408]\n",
      "Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(criterion='entropy')\n",
    "dt.fit(binarized_train_data, train_labels)\n",
    "print 'Using a decision stump -- a tree with depth 1:'\n",
    "print 'Feature importances:', dt.feature_importances_\n",
    "print 'Accuracy:', dt.score(binarized_test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a decision stump -- a tree with depth 1:\n",
      "Feature importances: [0. 0. 1. 0.]\n",
      "Accuracy: 0.66\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(criterion='entropy', max_depth=1)\n",
    "dt.fit(binarized_train_data, train_labels)\n",
    "print 'Using a decision stump -- a tree with depth 1:'\n",
    "print 'Feature importances:', dt.feature_importances_\n",
    "print 'Accuracy:', dt.score(binarized_test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've been using the binarized version of the iris features. Recall that we simply chose thresholds for each feature by inspecting feature histograms. Let's use information gain as a metric to choose a best feature and a best threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.708 0.525 sepal length (cm)\n",
      "1 3.309 0.311 sepal width (cm)\n",
      "2 1.910 0.893 petal length (cm)\n",
      "3 0.609 0.893 petal width (cm)\n"
     ]
    }
   ],
   "source": [
    "def try_features_and_thresholds(data, labels):\n",
    "    for feature in range(data.shape[1]):\n",
    "        # Choose a set of thresholds between the min- and max-valued feature, ignoring the min and max themselves.\n",
    "        thresholds = np.linspace(data[:,feature].min(), data[:,feature].max(), 100)[1:-1]\n",
    "\n",
    "        # Try each threshold and keep track of the best one for this feature.\n",
    "        best_threshold = 0\n",
    "        best_ig = 0\n",
    "        for threshold in thresholds:\n",
    "            ig = information_gain(data, labels, feature, threshold)\n",
    "            if ig > best_ig:\n",
    "                best_ig = ig\n",
    "                best_threshold = threshold\n",
    "\n",
    "        # Show the best threshold and information gain for this feature.\n",
    "        print '%d %.3f %.3f %s' %(feature, best_threshold, best_ig, iris.feature_names[feature])\n",
    "        \n",
    "try_features_and_thresholds(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like when we binarized our data, we didn't choose the thresholds that maximized information gain for 3 of 4 features. Let's try training actual decision trees (as opposed to stumps) with the original (non-binarized) data. You may need to install GraphViz before exporting the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function graph_from_dot_data in module pydot:\n",
      "\n",
      "graph_from_dot_data(s)\n",
      "    Load graphs from DOT description in string `s`.\n",
      "    \n",
      "    @param s: string in [DOT language](\n",
      "        https://en.wikipedia.org/wiki/DOT_(graph_description_language))\n",
      "    \n",
      "    @return: Graphs that result from parsing.\n",
      "    @rtype: `list` of `pydot.Dot`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pydot.graph_from_dot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Train a decision tree classifier.\n",
    "dt = DecisionTreeClassifier(criterion='entropy', min_samples_split=10)\n",
    "dt.fit(train_data, train_labels)\n",
    "print 'Accuracy:', dt.score(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: Tree Pages: 1 -->\r\n",
       "<svg width=\"368pt\" height=\"433pt\"\r\n",
       " viewBox=\"0.00 0.00 368.00 433.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 429)\">\r\n",
       "<title>Tree</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-429 364,-429 364,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<path fill=\"#8139e5\" fill-opacity=\"0.043137\" stroke=\"black\" d=\"M203,-425C203,-425 76,-425 76,-425 70,-425 64,-419 64,-413 64,-413 64,-354 64,-354 64,-348 70,-342 76,-342 76,-342 203,-342 203,-342 209,-342 215,-348 215,-354 215,-354 215,-413 215,-413 215,-419 209,-425 203,-425\"/>\r\n",
       "<text text-anchor=\"start\" x=\"72\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 0.8</text>\r\n",
       "<text text-anchor=\"start\" x=\"89.5\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 1.582</text>\r\n",
       "<text text-anchor=\"start\" x=\"92\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 100</text>\r\n",
       "<text text-anchor=\"start\" x=\"79\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [31, 33, 36]</text>\r\n",
       "<text text-anchor=\"start\" x=\"89.5\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M109,-298.5C109,-298.5 12,-298.5 12,-298.5 6,-298.5 0,-292.5 0,-286.5 0,-286.5 0,-242.5 0,-242.5 0,-236.5 6,-230.5 12,-230.5 12,-230.5 109,-230.5 109,-230.5 115,-230.5 121,-236.5 121,-242.5 121,-242.5 121,-286.5 121,-286.5 121,-292.5 115,-298.5 109,-298.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"18.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\r\n",
       "<text text-anchor=\"start\" x=\"17\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 31</text>\r\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [31, 0, 0]</text>\r\n",
       "<text text-anchor=\"start\" x=\"14.5\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M112.091,-341.907C104.492,-330.652 96.2312,-318.418 88.5931,-307.106\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"91.3914,-304.996 82.8948,-298.667 85.5901,-308.913 91.3914,-304.996\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"78.1364\" y=\"-319.51\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<path fill=\"#8139e5\" fill-opacity=\"0.082353\" stroke=\"black\" d=\"M286,-306C286,-306 151,-306 151,-306 145,-306 139,-300 139,-294 139,-294 139,-235 139,-235 139,-229 145,-223 151,-223 151,-223 286,-223 286,-223 292,-223 298,-229 298,-235 298,-235 298,-294 298,-294 298,-300 292,-306 286,-306\"/>\r\n",
       "<text text-anchor=\"start\" x=\"147\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.65</text>\r\n",
       "<text text-anchor=\"start\" x=\"168.5\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.999</text>\r\n",
       "<text text-anchor=\"start\" x=\"175\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 69</text>\r\n",
       "<text text-anchor=\"start\" x=\"162\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 33, 36]</text>\r\n",
       "<text text-anchor=\"start\" x=\"168.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>0&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M166.909,-341.907C172.914,-333.014 179.331,-323.509 185.529,-314.331\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"188.444,-316.267 191.14,-306.021 182.643,-312.35 188.444,-316.267\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"195.898\" y=\"-326.864\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 3 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\r\n",
       "<path fill=\"#39e581\" fill-opacity=\"0.878431\" stroke=\"black\" d=\"M209,-187C209,-187 68,-187 68,-187 62,-187 56,-181 56,-175 56,-175 56,-116 56,-116 56,-110 62,-104 68,-104 68,-104 209,-104 209,-104 215,-104 221,-110 221,-116 221,-116 221,-175 221,-175 221,-181 215,-187 209,-187\"/>\r\n",
       "<text text-anchor=\"start\" x=\"64\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.95</text>\r\n",
       "<text text-anchor=\"start\" x=\"88.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.494</text>\r\n",
       "<text text-anchor=\"start\" x=\"95\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 37</text>\r\n",
       "<text text-anchor=\"start\" x=\"86\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 33, 4]</text>\r\n",
       "<text text-anchor=\"start\" x=\"83\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;3 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>2&#45;&gt;3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M190.744,-222.907C184.663,-214.014 178.164,-204.509 171.889,-195.331\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"174.74,-193.3 166.207,-187.021 168.962,-197.251 174.74,-193.3\"/>\r\n",
       "</g>\r\n",
       "<!-- 6 -->\r\n",
       "<g id=\"node7\" class=\"node\"><title>6</title>\r\n",
       "<path fill=\"#8139e5\" stroke=\"black\" d=\"M348,-179.5C348,-179.5 251,-179.5 251,-179.5 245,-179.5 239,-173.5 239,-167.5 239,-167.5 239,-123.5 239,-123.5 239,-117.5 245,-111.5 251,-111.5 251,-111.5 348,-111.5 348,-111.5 354,-111.5 360,-117.5 360,-123.5 360,-123.5 360,-167.5 360,-167.5 360,-173.5 354,-179.5 348,-179.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"257.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\r\n",
       "<text text-anchor=\"start\" x=\"256\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 32</text>\r\n",
       "<text text-anchor=\"start\" x=\"247\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 32]</text>\r\n",
       "<text text-anchor=\"start\" x=\"249.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;6 -->\r\n",
       "<g id=\"edge6\" class=\"edge\"><title>2&#45;&gt;6</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M246.603,-222.907C254.395,-211.652 262.864,-199.418 270.696,-188.106\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"273.724,-189.881 276.538,-179.667 267.968,-185.897 273.724,-189.881\"/>\r\n",
       "</g>\r\n",
       "<!-- 4 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\r\n",
       "<path fill=\"#39e581\" stroke=\"black\" d=\"M120,-68C120,-68 17,-68 17,-68 11,-68 5,-62 5,-56 5,-56 5,-12 5,-12 5,-6 11,-0 17,-0 17,-0 120,-0 120,-0 126,-0 132,-6 132,-12 132,-12 132,-56 132,-56 132,-62 126,-68 120,-68\"/>\r\n",
       "<text text-anchor=\"start\" x=\"26.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\r\n",
       "<text text-anchor=\"start\" x=\"25\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 32</text>\r\n",
       "<text text-anchor=\"start\" x=\"16\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 32, 0]</text>\r\n",
       "<text text-anchor=\"start\" x=\"13\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;4 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>3&#45;&gt;4</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M112.435,-103.726C106.837,-94.9703 100.913,-85.7032 95.2886,-76.9051\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"98.1226,-74.8399 89.7874,-68.2996 92.2247,-78.6103 98.1226,-74.8399\"/>\r\n",
       "</g>\r\n",
       "<!-- 5 -->\r\n",
       "<g id=\"node6\" class=\"node\"><title>5</title>\r\n",
       "<path fill=\"#8139e5\" fill-opacity=\"0.749020\" stroke=\"black\" d=\"M254.5,-68C254.5,-68 162.5,-68 162.5,-68 156.5,-68 150.5,-62 150.5,-56 150.5,-56 150.5,-12 150.5,-12 150.5,-6 156.5,-0 162.5,-0 162.5,-0 254.5,-0 254.5,-0 260.5,-0 266.5,-6 266.5,-12 266.5,-12 266.5,-56 266.5,-56 266.5,-62 260.5,-68 254.5,-68\"/>\r\n",
       "<text text-anchor=\"start\" x=\"158.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.722</text>\r\n",
       "<text text-anchor=\"start\" x=\"169\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 5</text>\r\n",
       "<text text-anchor=\"start\" x=\"160\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 4]</text>\r\n",
       "<text text-anchor=\"start\" x=\"158.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;5 -->\r\n",
       "<g id=\"edge5\" class=\"edge\"><title>3&#45;&gt;5</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M164.565,-103.726C170.163,-94.9703 176.087,-85.7032 181.711,-76.9051\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"184.775,-78.6103 187.213,-68.2996 178.877,-74.8399 184.775,-78.6103\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x1775e550>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_data = tree.export_graphviz(dt, out_file=None, \n",
    "                         feature_names=iris.feature_names,  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you successfully output the tree, you should be able to see it here. The first split perfectly partitions the setosas because they have very narrow petals. The next split identifies a pure subset of virginicas that have wide petals. Of the remaining medium-width petal examples, those with shorter petals are versicolors, but the split is not perfect. At this point, we stop splitting because we don't have enough samples to be convinced that further splitting would generalize well.\n",
    "\n",
    "Note, though, that his depth 3 tree gets 96% accuracy on the test data. So does a depth 2 tree (try it!). Tree pruning, which is not implemented in sklearn, can be useful for choosing a depth that generalizes well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
